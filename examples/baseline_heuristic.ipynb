{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Baseline Heuristic\n",
                "\n",
                "Simple heuristic baselines for comparison:\n",
                "- Random guessing\n",
                "- Majority class\n",
                "- Effect size threshold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "from collections import Counter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('../data/splits/openai_subsets/p3sa_500.jsonl') as f:\n",
                "    samples = [json.loads(line) for line in f]\n",
                "\n",
                "print(f\"Total samples: {len(samples)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Baseline 1: Random Guessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "\n",
                "random_preds = np.random.choice(['yes', 'no'], size=len(samples))\n",
                "ground_truth = [s['binary_answer'] for s in samples]\n",
                "\n",
                "random_acc = np.mean([p == g for p, g in zip(random_preds, ground_truth)])\n",
                "print(f\"Random baseline accuracy: {random_acc:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Baseline 2: Majority Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "label_counts = Counter(ground_truth)\n",
                "majority_class = label_counts.most_common(1)[0][0]\n",
                "\n",
                "print(f\"Label distribution: {dict(label_counts)}\")\n",
                "print(f\"Majority class: '{majority_class}'\")\n",
                "\n",
                "majority_acc = label_counts[majority_class] / len(samples)\n",
                "print(f\"Majority baseline accuracy: {majority_acc:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Baseline 3: Effect Size Threshold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_by_effect_size(sample, threshold=0.5):\n",
                "    \"\"\"Predict 'yes' if |effect_size| > threshold.\"\"\"\n",
                "    effect = abs(sample.get('effect_size', 0))\n",
                "    return 'yes' if effect > threshold else 'no'\n",
                "\n",
                "effect_preds = [predict_by_effect_size(s) for s in samples]\n",
                "effect_acc = np.mean([p == g for p, g in zip(effect_preds, ground_truth)])\n",
                "\n",
                "print(f\"Effect-size threshold accuracy: {effect_acc:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== Baseline Comparison ===\")\n",
                "print(f\"Random:       {random_acc:.1%}\")\n",
                "print(f\"Majority:     {majority_acc:.1%}\")\n",
                "print(f\"Effect-size:  {effect_acc:.1%}\")\n",
                "print(\"\\nLLM models should significantly outperform these baselines.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}