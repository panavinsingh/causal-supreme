{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Quick Evaluation with GPT-4o\n",
                "\n",
                "Demonstrates how to evaluate an LLM on the Causal-Supreme dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "# Set your API key\n",
                "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Test Samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('../data/splits/openai_subsets/p3sa_500.jsonl') as f:\n",
                "    samples = [json.loads(line) for line in f][:10]  # First 10 for demo\n",
                "\n",
                "print(f\"Loaded {len(samples)} samples for evaluation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SYSTEM_PROMPT = \"\"\"You are an expert in causal reasoning. Answer the following question about causal relationships.\n",
                "Respond with ONLY 'yes' or 'no'.\"\"\"\n",
                "\n",
                "def evaluate_sample(sample):\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-4o-mini\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "            {\"role\": \"user\", \"content\": sample['nl_prompt']}\n",
                "        ],\n",
                "        max_tokens=10,\n",
                "        temperature=0\n",
                "    )\n",
                "    \n",
                "    answer = response.choices[0].message.content.strip().lower()\n",
                "    ground_truth = sample['binary_answer'].lower()\n",
                "    \n",
                "    return {\n",
                "        'rung': sample['query']['rung'],\n",
                "        'prediction': answer,\n",
                "        'ground_truth': ground_truth,\n",
                "        'correct': answer == ground_truth\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation (uncomment to execute - costs API credits)\n",
                "# results = [evaluate_sample(s) for s in samples]\n",
                "# accuracy = sum(r['correct'] for r in results) / len(results)\n",
                "# print(f\"Accuracy: {accuracy:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example Output\n",
                "\n",
                "```\n",
                "Accuracy: 65.0%\n",
                "Per-rung:\n",
                "  Rung 1 (Association): 80%\n",
                "  Rung 2 (Intervention): 60%\n",
                "  Rung 3 (Counterfactual): 55%\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}