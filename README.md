# Causal-Supreme

[![CI](https://github.com/YOUR_USERNAME/causal-supreme/actions/workflows/ci.yml/badge.svg)](https://github.com/YOUR_USERNAME/causal-supreme/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/Code-MIT-blue.svg)](LICENSE)
[![License: CC BY 4.0](https://img.shields.io/badge/Data-CC--BY--4.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Dataset Card](https://img.shields.io/badge/ü§ó-Dataset_Card-yellow.svg)](data/metadata/DATASET_CARD.md)

A research-grade synthetic dataset for evaluating causal reasoning in large language models, covering all three rungs of Pearl's Ladder of Causation.

> üìã **[Full Dataset Card](data/metadata/DATASET_CARD.md)** ‚Äî HuggingFace-compatible documentation

## Project Overview

Causal-Supreme provides:
- **Structural Causal Models (SCMs)** with random DAG topologies (5-20 nodes)
- **Pearl's Ladder Coverage**: Association, Intervention, and Counterfactual queries
- **Guaranteed Integrity**: Zero DAG overlap between splits, KS-compliant distributions
- **LLM Evaluation Subsets**: Pre-stratified samples for cost-efficient API evaluation

```
causal-supreme/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Generator outputs (.jsonl shards)
‚îÇ   ‚îú‚îÄ‚îÄ splits/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl         # ~26,400 samples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val.jsonl           # ~3,300 samples  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test.jsonl          # ~3,300 samples
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ openai_subsets/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ cladder_600.csv # CLadder benchmark (200/rung)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ p3sa_500.jsonl  # P3SA synthetic (~167/rung)
‚îÇ   ‚îî‚îÄ‚îÄ metadata/
‚îÇ       ‚îî‚îÄ‚îÄ DATASET_CARD.md     # HuggingFace template
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ generator/              # DAG + SCM generation
‚îÇ   ‚îú‚îÄ‚îÄ splitting/              # Hash-based splitting + KS fix
‚îÇ   ‚îî‚îÄ‚îÄ validation/             # Pytest integrity suite
‚îÇ
‚îú‚îÄ‚îÄ examples/                   # Usage notebooks
‚îÇ   ‚îú‚îÄ‚îÄ sample_view.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ quick_eval_gpt4o.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ baseline_heuristic.ipynb
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # Unit tests
‚îú‚îÄ‚îÄ configs/                    # YAML configurations
‚îî‚îÄ‚îÄ scripts/                    # Utility scripts
```

## Quick Start

### Environment Setup

```bash
# Option 1: Conda (recommended)
conda env create -f environment.yml
conda activate causal-supreme

# Option 2: pip
pip install -r requirements.txt
# Or for exact reproducibility:
pip install -r requirements-lock.txt
```

### Regenerate Dataset (Deterministic)

For **exact reproducibility**, set environment variables before generation:

```bash
# Ensure deterministic hashing and seeding
export PYTHONHASHSEED=0
export CSD_SEED=42

# Generate raw data (1100 DAGs √ó 30 samples = 33,000)
python src/generator/generate_dataset.py \
    --conf configs/default.yaml \
    --n-dags 1100 \
    --samples-per-dag 30 \
    --out data/raw/shard_000.jsonl \
    --seed $CSD_SEED

# Create splits with KS correction
python src/splitting/create_splits.py --ks-fix --iterations 5

# Run integrity tests
pytest -q
```

### Using the Dataset

```python
import json

# Load P3SA test subset
with open("data/splits/openai_subsets/p3sa_500.jsonl") as f:
    samples = [json.loads(line) for line in f]

for sample in samples[:3]:
    print(f"Rung: {sample['query']['rung']}")
    print(f"Prompt: {sample['nl_prompt']}")
    print(f"Answer: {sample['binary_answer']}\n")
```

See [`examples/`](examples/) for complete usage notebooks.

## OpenAI Evaluation Subsets

| Dataset | Samples | Per Rung | Format |
|---------|---------|----------|--------|
| CLadder | 600 | 200 | CSV |
| P3SA Synthetic | 498 | ~166 | JSONL |

### Cost Estimation

**Rough API cost** = `(tokens_in √ó $rate_in) + (tokens_out √ó $rate_out)`  
We cap completions at 2,000 tokens, giving approximately **$0.15** for 1,098 samples with GPT-4o-mini.

> ‚ö†Ô∏è **Note:** This is a *ball-park* estimate; real usage can vary ¬±25% depending on tokenisation and prompt formatting.

## Data Structure

Each P3SA sample contains:

```json
{
  "dag_id": "dag_00042",
  "dag": {"nodes": 10, "edges": [[0,1,0.5], ...]},
  "query": {
    "rung": 3,
    "type": "counterfactual",
    "treatment_node": 2,
    "outcome_node": 7,
    "intervention_value": 1.5
  },
  "X_factual": [0.1, -0.5, ...],
  "X_counterfactual": [0.1, 1.5, ...],
  "effect_size": 0.613,
  "binary_answer": "yes",
  "nl_prompt": "In a system with 10 causally related variables...",
  "nl_answer": "Yes, the intervention would increase..."
}
```

## Integrity Guarantees

- ‚úÖ **Zero DAG Overlap**: Hash-based splitting ensures no structural leakage
- ‚úÖ **KS Compliance**: Micro-resampling aligns train/test distributions  
- ‚úÖ **Balanced Rungs**: ~33% samples per Pearl's Ladder level
- ‚úÖ **Motif Coverage**: Every DAG contains fork, collider, and chain structures

## Citation

If you use this dataset, please cite:

```bibtex
@dataset{causal_supreme_2026,
  title     = {Causal-Supreme: A Synthetic Benchmark for Causal Reasoning in LLMs},
  author    = {panavinsingh},
  year      = {2026},
  publisher = {GitHub},
  version   = {1.0.0},
  url       = {https://github.com/panavinsingh/causal-supreme},
  note      = {Dataset for evaluating Pearl's Ladder reasoning}
}
```



## License

- **Code**: MIT License
- **Data**: CC-BY-4.0

See [LICENSE](LICENSE) for full text.
